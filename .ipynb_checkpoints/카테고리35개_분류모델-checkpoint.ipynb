{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4425ad8c",
   "metadata": {},
   "source": [
    "# 파일 데이터 가져와서 라벨 붙이기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791f278d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경로확인 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7ae47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터가 있는 경로로 이동"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966d58dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 리스팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7866ac51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 반복문 이용해서 데이터 합치기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c70fa9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델이랑 피클 저장할 경로로 바꿔주기\n",
    "# ./카테고리분류모델피클 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931ca2c2",
   "metadata": {},
   "source": [
    "# 토크나이징 하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfd3586",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571028b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(docs):\n",
    "    tokenizer = RegexpTokenizer('[\\w]+')\n",
    "    stop_words = stopwords.words('english')\n",
    "    p_stemmer = PorterStemmer()  # 어근 복원 : runs, running, ran => run,  cars => car\n",
    "#     p = re.compile(\"[0-9]+\")\n",
    "#     숫자제거해야됨\n",
    "#     docs = [p.sub(i) for i in docs]\n",
    "    docs = BeautifulSoup(docs, \"html5lib\").get_text()\n",
    "    low = docs.lower()\n",
    "    tokens = tokenizer.tokenize(low)\n",
    "    stopped_tokens = [i for i in tokens if not i in stop_words]\n",
    "    stopped_tokens = [re.sub('[^a-zA-Z0-9]',' ',i) for i in stopped_tokens]\n",
    "    stemmer_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    one_rmv = [w for w in stemmer_tokens if len(w)>1]\n",
    "  \n",
    "    return one_rmv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430d4412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 함수 적용 \n",
    "# df['other_columns'] = df['columns'].apply(preprocessing)\n",
    "\n",
    "# @@토큰화 하기전 컬럼 보존 @@"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63160a6",
   "metadata": {},
   "source": [
    "# 전처리 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db313344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 할 변수 만들어 주기 \n",
    "# preprocessed_sentences = df['columns']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548f5995",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# fit_on_texts()안에 코퍼스를 입력으로 하면 빈도수를 기준으로 단어 집합을 생성.\n",
    "# tokenizer.fit_on_texts(preprocessed_sentences) \n",
    "vocab_size = 10000\n",
    "tokenizer = Tokenizer(num_words = vocab_size + 1) \n",
    "tokenizer.fit_on_texts(preprocessed_sentences)\n",
    "\n",
    "# 상위 1만개 단어 사용 해서 빈도수 기준으로 단어 집합 생성 하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c0e046",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit_on_texts로 토큰화 한 것을 pickle로 저장 \n",
    "import pickle\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805a9d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle파일이 현재 디렉토리에 저장됨 \n",
    "import pickle\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e58447a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어를 시퀀스 형태로 변환해주기 \n",
    "x = tokenizer.texts_to_sequences(preprocessed_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b88b35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련시킬 할 라벨 지정 \n",
    "y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90128968",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d283bb2b",
   "metadata": {},
   "source": [
    "# 모델 돌리기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adeab88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련시킬 데이터 셋 나누기\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194628b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기사최대길이로 패딩해주기 \n",
    "max_len = max([len(l) for l in df['토큰화한기사']])\n",
    "\n",
    "x_train = pad_sequences(x_train, maxlen=max_len)\n",
    "x_test = pad_sequences(x_test,maxlen=max_len)\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6345e981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM 모델 구성\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "embedding_dim = 128\n",
    "hidden_units = 128\n",
    "num_classes = 5\n",
    "\n",
    "with tf.device('/GPU:0'):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embedding_dim))\n",
    "    model.add(LSTM(hidden_units))\n",
    "    model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f966ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20)\n",
    "# 에포크20때까지 val_acc가 개선되지않으면 스톱 \n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "# 제일 훈련이 잘된것을 현재경로에 저장 \n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "history = model.fit(x_train, y_train, batch_size=32, epochs=30, callbacks=[es, mc], validation_data=(x_test, y_test) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59e529d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 시각화 \n",
    "epochs = range(1, len(history.history['acc']) + 1)\n",
    "plt.plot(epochs, history.history['loss'])\n",
    "plt.plot(epochs, history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "epochs = range(1, len(history.history['acc']) + 1)\n",
    "plt.plot(epochs, history.history['acc'])\n",
    "plt.plot(epochs, history.history['val_acc'])\n",
    "plt.title('model acc')\n",
    "plt.ylabel('acc')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81eafa4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e44416",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1455d32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12025e00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f9856e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097f0fd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41857ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12fba5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
