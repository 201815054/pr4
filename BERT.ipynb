{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00f0171c",
   "metadata": {},
   "source": [
    "# BERT\n",
    "- pretraing된 모델임 , 트랜스포머에서 사용한 인코더부분을 쌓아놓음 \n",
    "- 엘모처럼 양방향으로 학습을 시킴\n",
    "- 트랜스포머의 인코더를 12층, 4층을 쌓아서 만든 버트가 있음 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfab7d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어떤단어가있을때 토크나이징을 하면 캐릭터단위로나옴(원그램) 두개로하면 (투그램) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ef6884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"19-2. kor_bert_nsmc_tpu.ipynb\n",
    "\n",
    "Automatically generated by Colaboratory.\n",
    "\n",
    "Original file is located at\n",
    "    https://colab.research.google.com/drive/1ehYQYbaWYk8l6AJANTT6q62MIiSB3gh0\n",
    "\"\"\"\n",
    "\n",
    "pip install transformers\n",
    "\n",
    "import transformers\n",
    "transformers.__version__\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "\n",
    "# logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\", filename=\"ratings_train.txt\")\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\", filename=\"ratings_test.txt\")\n",
    "\n",
    "train_data = pd.read_table('ratings_train.txt')\n",
    "test_data = pd.read_table('ratings_test.txt')\n",
    "\n",
    "print('훈련용 리뷰 개수 :',len(train_data)) # 훈련용 리뷰 개수 출력\n",
    "\n",
    "print('테스트용 리뷰 개수 :',len(test_data)) # 테스트용 리뷰 개수 출력\n",
    "\n",
    "train_data[:5] # 상위 5개 출력\n",
    "\n",
    "test_data[:5] # 상위 5개 출력\n",
    "\n",
    "train_data = train_data.dropna(how = 'any') # Null 값이 존재하는 행 제거\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "print(train_data.isnull().values.any()) # Null 값이 존재하는지 확인\n",
    "\n",
    "test_data = test_data.dropna(how = 'any') # Null 값이 존재하는 행 제거\n",
    "test_data = test_data.reset_index(drop=True)\n",
    "print(test_data.isnull().values.any()) # Null 값이 존재하는지 확인\n",
    "\n",
    "print(len(train_data))\n",
    "\n",
    "print(len(test_data))\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('klue/bert-base')\n",
    "\n",
    "print(tokenizer.encode(\"보는내내 그대로 들어맞는 예측 카리스마 없는 악역\"))\n",
    "\n",
    "print(tokenizer.tokenize(\"보는내내 그대로 들어맞는 예측 카리스마 없는 악역\"))\n",
    "\n",
    "tokenizer.decode(tokenizer.encode(\"보는내내 그대로 들어맞는 예측 카리스마 없는 악역\"))\n",
    "\n",
    "for elem in tokenizer.encode(\"보는내내 그대로 들어맞는 예측 카리스마 없는 악역\"):\n",
    "  print(tokenizer.decode(elem))\n",
    "\n",
    "print(tokenizer.tokenize(\"전율을 일으키는 영화. 다시 보고싶은 영화\"))\n",
    "\n",
    "print(tokenizer.encode(\"전율을 일으키는 영화. 다시 보고싶은 영화\"))\n",
    "\n",
    "for elem in tokenizer.encode(\"전율을 일으키는 영화. 다시 보고싶은 영화\"):\n",
    "  print(tokenizer.decode(elem))\n",
    "\n",
    "for elem in tokenizer.encode(\"happy birthday~!\"):\n",
    "  print(tokenizer.decode(elem))\n",
    "\n",
    "print(tokenizer.decode(2))\n",
    "\n",
    "print(tokenizer.decode(3))\n",
    "\n",
    "print(tokenizer.cls_token, ':', tokenizer.cls_token_id)\n",
    "print(tokenizer.sep_token, ':' , tokenizer.sep_token_id)\n",
    "\n",
    "print(tokenizer.pad_token, ':', tokenizer.pad_token_id)\n",
    "\n",
    "max_seq_len = 128\n",
    "\n",
    "encoded_result = tokenizer.encode(\"전율을 일으키는 영화. 다시 보고싶은 영화\", max_length=max_seq_len, pad_to_max_length=True)\n",
    "print(encoded_result)\n",
    "print('길이 :', len(encoded_result))\n",
    "\n",
    "# 세그멘트 인풋\n",
    "print([0]*max_seq_len)\n",
    "\n",
    "# 마스크 인풋\n",
    "valid_num = len(tokenizer.encode(\"전율을 일으키는 영화. 다시 보고싶은 영화\"))\n",
    "print(valid_num * [1] + (max_seq_len - valid_num) * [0])\n",
    "\n",
    "def convert_examples_to_features(examples, labels, max_seq_len, tokenizer):\n",
    "    \n",
    "    input_ids, attention_masks, token_type_ids, data_labels = [], [], [], []\n",
    "    \n",
    "    for example, label in tqdm(zip(examples, labels), total=len(examples)):\n",
    "        input_id = tokenizer.encode(example, max_length=max_seq_len, pad_to_max_length=True)\n",
    "        padding_count = input_id.count(tokenizer.pad_token_id)\n",
    "        attention_mask = [1] * (max_seq_len - padding_count) + [0] * padding_count\n",
    "        token_type_id = [0] * max_seq_len\n",
    "\n",
    "        assert len(input_id) == max_seq_len, \"Error with input length {} vs {}\".format(len(input_id), max_seq_len)\n",
    "        assert len(attention_mask) == max_seq_len, \"Error with attention mask length {} vs {}\".format(len(attention_mask), max_seq_len)\n",
    "        assert len(token_type_id) == max_seq_len, \"Error with token type length {} vs {}\".format(len(token_type_id), max_seq_len)\n",
    "\n",
    "        input_ids.append(input_id)\n",
    "        attention_masks.append(attention_mask)\n",
    "        token_type_ids.append(token_type_id)\n",
    "        data_labels.append(label)\n",
    "\n",
    "    input_ids = np.array(input_ids, dtype=int)\n",
    "    attention_masks = np.array(attention_masks, dtype=int)\n",
    "    token_type_ids = np.array(token_type_ids, dtype=int)\n",
    "\n",
    "    data_labels = np.asarray(data_labels, dtype=np.int32)\n",
    "\n",
    "    return (input_ids, attention_masks, token_type_ids), data_labels\n",
    "\n",
    "train_X, train_y = convert_examples_to_features(train_data['document'], train_data['label'], max_seq_len=max_seq_len, tokenizer=tokenizer)\n",
    "\n",
    "test_X, test_y = convert_examples_to_features(test_data['document'], test_data['label'], max_seq_len=max_seq_len, tokenizer=tokenizer)\n",
    "\n",
    "# 최대 길이: 128\n",
    "input_id = train_X[0][0]\n",
    "attention_mask = train_X[1][0]\n",
    "token_type_id = train_X[2][0]\n",
    "label = train_y[0]\n",
    "\n",
    "print('단어에 대한 정수 인코딩 :',input_id)\n",
    "print('어텐션 마스크 :',attention_mask)\n",
    "print('세그먼트 인코딩 :',token_type_id)\n",
    "print('각 인코딩의 길이 :', len(input_id))\n",
    "print('정수 인코딩 복원 :',tokenizer.decode(input_id))\n",
    "print('레이블 :',label)\n",
    "\n",
    "model = TFBertModel.from_pretrained(\"klue/bert-base\", from_pt=True)\n",
    "\n",
    "max_seq_len = 128\n",
    "\n",
    "input_ids_layer = tf.keras.layers.Input(shape=(max_seq_len,), dtype=tf.int32)\n",
    "attention_masks_layer = tf.keras.layers.Input(shape=(max_seq_len,), dtype=tf.int32)\n",
    "token_type_ids_layer = tf.keras.layers.Input(shape=(max_seq_len,), dtype=tf.int32)\n",
    "\n",
    "outputs = model([input_ids_layer, attention_masks_layer, token_type_ids_layer])\n",
    "\n",
    "print(outputs)\n",
    "\n",
    "print(outputs[0])\n",
    "\n",
    "print(outputs[1])\n",
    "\n",
    "class TFBertForSequenceClassification(tf.keras.Model):\n",
    "    def __init__(self, model_name):\n",
    "        super(TFBertForSequenceClassification, self).__init__()\n",
    "        self.bert = TFBertModel.from_pretrained(model_name, from_pt=True)\n",
    "        self.classifier = tf.keras.layers.Dense(1,\n",
    "                                                kernel_initializer=tf.keras.initializers.TruncatedNormal(0.02),\n",
    "                                                activation='sigmoid',\n",
    "                                                name='classifier')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input_ids, attention_mask, token_type_ids = inputs\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        cls_token = outputs[1]\n",
    "        prediction = self.classifier(cls_token)\n",
    "\n",
    "        return prediction\n",
    "\n",
    "\"\"\"TPU 사용법 : https://wikidocs.net/119990\"\"\"\n",
    "\n",
    "# TPU 작동을 위한 코드 TPU 작동을 위한 코드\n",
    "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
    "tf.config.experimental_connect_to_cluster(resolver)\n",
    "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "\n",
    "strategy = tf.distribute.experimental.TPUStrategy(resolver)\n",
    "\n",
    "with strategy.scope():\n",
    "  model = TFBertForSequenceClassification(\"klue/bert-base\")\n",
    "  optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "  loss = tf.keras.losses.BinaryCrossentropy()\n",
    "  model.compile(optimizer=optimizer, loss=loss, metrics = ['accuracy'])\n",
    "\n",
    "model.fit(train_X, train_y, epochs=2, batch_size=64, validation_split=0.2)\n",
    "\n",
    "results = model.evaluate(test_X, test_y, batch_size=1024)\n",
    "print(\"test loss, test acc: \", results)\n",
    "\n",
    "def sentiment_predict(new_sentence):\n",
    "  input_id = tokenizer.encode(new_sentence, max_length=max_seq_len, pad_to_max_length=True)\n",
    "\n",
    "  padding_count = input_id.count(tokenizer.pad_token_id)\n",
    "  attention_mask = [1] * (max_seq_len - padding_count) + [0] * padding_count\n",
    "  token_type_id = [0] * max_seq_len\n",
    "\n",
    "  input_ids = np.array([input_id])\n",
    "  attention_masks = np.array([attention_mask])\n",
    "  token_type_ids = np.array([token_type_id])\n",
    "\n",
    "  encoded_input = [input_ids, attention_masks, token_type_ids]\n",
    "  score = model.predict(encoded_input)[0][0]\n",
    "\n",
    "  if(score > 0.5):\n",
    "    print(\"{:.2f}% 확률로 긍정 리뷰입니다.\\n\".format(score * 100))\n",
    "  else:\n",
    "    print(\"{:.2f}% 확률로 부정 리뷰입니다.\\n\".format((1 - score) * 100))\n",
    "\n",
    "sentiment_predict('보던거라 계속보고있는데 전개도 느리고 주인공인 은희는 한두컷 나오면서 소극적인모습에 ')\n",
    "\n",
    "sentiment_predict(\"스토리는 확실히 실망이였지만 배우들 연기력이 대박이였다 특히 이제훈 연기 정말 ... 이 배우들로 이렇게밖에 만들지 못한 영화는 아쉽지만 배우들 연기력과 사운드는 정말 빛났던 영화. 기대하고 극장에서 보면 많이 실망했겠지만 평점보고 기대없이 집에서 편하게 보면 괜찮아요. 이제훈님 연기력은 최고인 것 같습니다\")\n",
    "\n",
    "sentiment_predict(\"남친이 이 영화를 보고 헤어지자고한 영화. 자유롭게 살고 싶다고 한다. 내가 무슨 나비를 잡은 덫마냥 나에겐 다시 보고싶지 않은 영화.\")\n",
    "\n",
    "sentiment_predict(\"이 영화 존잼입니다 대박\")\n",
    "\n",
    "sentiment_predict('이 영화 개꿀잼 ㅋㅋㅋ')\n",
    "\n",
    "sentiment_predict('이 영화 핵노잼 ㅠㅠ')\n",
    "\n",
    "sentiment_predict('이딴게 영화냐 ㅉㅉ')\n",
    "\n",
    "sentiment_predict('감독 뭐하는 놈이냐?')\n",
    "\n",
    "sentiment_predict('와 개쩐다 정말 세계관 최강자들의 영화다')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
